Initial working directory: /gpfs/home6/rberger/rlvr/GRPO_prompt_replay/open-instruct
Defaulting to user installation because normal site-packages is not writeable
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
Requirement already satisfied: uv in /home/rberger/.local/lib/python3.12/site-packages (0.9.5)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
Resolved 235 packages in 3ms
Audited 230 packages in 125ms
Defaulting to user installation because normal site-packages is not writeable
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
Requirement already satisfied: uv in /home/rberger/.local/lib/python3.12/site-packages (0.9.5)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
WARNING: Ignoring invalid distribution ~lash-attn (/home/rberger/.local/lib/python3.12/site-packages)
Resolved 235 packages in 1ms
Audited 230 packages in 9ms
[info] Using repo root: /home/rberger/rlvr
[info] Output dir: /scratch-shared/rberber/rlvr/outputs/qwen25-math-rlzero-math/checkpoints
[info] Local eval subset sample count: 256
[info] Benchmark dataset: HuggingFaceH4/MATH-500 128
[info] Benchmark splits: test
[2025-12-07 13:06:51,253] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/gpfs/home6/rberger/rlvr/open-instruct/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
INFO 12-07 13:07:00 [__init__.py:216] Automatically detected platform cuda.
wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: s4422090 (RL_seminar). Use `wandb login --relogin` to force relogin
2025-12-07 13:07:12 - INFO - grpo_fast.py:2942 - Configured local eval subset using 256 prompts across 1 training datasets
wandb: Tracking run with wandb version 0.18.1
wandb: Run data is saved locally in /gpfs/home6/rberger/rlvr/open-instruct/wandb/run-20251207_130712-xzki8kb6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run replay_8000_16rollouts_20cooldown___123__1765109231
wandb: â­ï¸ View project at https://wandb.ai/RL_seminar/olmo3_curriculum
wandb: ðŸš€ View run at https://wandb.ai/RL_seminar/olmo3_curriculum/runs/xzki8kb6
2025-12-07 13:07:16 - INFO - grpo_fast.py:2997 - Loading benchmark dataset: ['HuggingFaceH4/MATH-500', '128']
2025-12-07 13:07:17 - INFO - grpo_fast.py:3013 - Benchmark dataset loaded with 127 examples
by default, we will use the same split for all datasets
Dataset saurabh5/DAPO-Math-17k-Processed_filtered_olmo_completions_new_template_filtered: 10385 -> 8000 samples (factor: 8000)
âœ… Found cached dataset at /gpfs/home6/rberger/rlvr/open-instruct/local_dataset_cache/d3d133e49f
Dataset saurabh5/DAPO-Math-17k-Processed_filtered_olmo_completions_new_template_filtered: 10385 -> 256 samples (factor: 256)
âœ… Found cached dataset at /gpfs/home6/rberger/rlvr/open-instruct/local_dataset_cache/ddeda792bb
by default, we will use the same split for all datasets
Dataset HuggingFaceH4/MATH-500: 500 -> 128 samples (factor: 128)
âœ… Found cached dataset at /gpfs/home6/rberger/rlvr/open-instruct/local_dataset_cache/7f968f0bdb
<|im_start|>system
Please reason step by step, and put your final answer within \boxed{}.<|im_end|>
<|im_start|>user
A moving particle starts at the point $(4,4)$ and moves until it hits one of the
coordinate axes for the first time. When the particle is at the point $(a,b)$ , 
it moves at random to one of the points $(a-1,b)$ , $(a,b-1)$ , or $(a-1,b-1)$ ,
each with probability $\frac{1}{3}$ , independently of its previous moves. The 
probability that it will hit the coordinate axes at $(0,0)$ is $\frac{m}{3^n}$ ,
where $m$ and $n$ are positive integers such that $m$ is not divisible by $3$ . 
Find $m + n$ .<|im_end|>
<|im_start|>assistant

[
â”‚   Args(
â”‚   â”‚   dataset_mixer_list=[
â”‚   â”‚   â”‚   'saurabh5/DAPO-Math-17k-Processed_filtered_olmo_completions_new_template_filtered',
â”‚   â”‚   â”‚   '8000'
â”‚   â”‚   ],
â”‚   â”‚   dataset_mixer_eval_list=[
â”‚   â”‚   â”‚   'saurabh5/DAPO-Math-17k-Processed_filtered_olmo_completions_new_template_filtered',
â”‚   â”‚   â”‚   '256'
â”‚   â”‚   ],
â”‚   â”‚   dataset_mixer_list_splits=['train'],
â”‚   â”‚   dataset_mixer_eval_list_splits=['train', 'train'],
â”‚   â”‚   local_eval_subset_sample_count=256,
â”‚   â”‚   local_eval_timeout=300.0,
â”‚   â”‚   dataset_mixer_benchmark_list=['HuggingFaceH4/MATH-500', '128'],
â”‚   â”‚   dataset_mixer_benchmark_list_splits=['test'],
â”‚   â”‚   benchmark_eval_every=-1,
â”‚   â”‚   dataset_config_benchmark_hash=None,
â”‚   â”‚   dataset_transform_fn=['rlvr_tokenize_v1', 'rlvr_max_length_filter_v1'],
â”‚   â”‚   dataset_cache_mode='local',
â”‚   â”‚   dataset_local_cache_dir='/gpfs/home6/rberger/rlvr/open-instruct/local_dataset_cache',
â”‚   â”‚   dataset_config_hash=None,
â”‚   â”‚   dataset_config_eval_hash=None,
â”‚   â”‚   dataset_skip_cache=False,
â”‚   â”‚   shuffle_eval_dataset=False,
â”‚   â”‚   max_prompt_token_length=512,
â”‚   â”‚   system_prompt_override_file=None,
â”‚   â”‚   enable_prompt_pass_curriculum=False,
â”‚   â”‚   zero_pass_curriculum_fraction=0.25,
â”‚   â”‚   prompt_pass_curriculum_05sort=True,
â”‚   â”‚   enable_prompt_replay=True,
â”‚   â”‚   prompt_replay_fraction=0.5,
â”‚   â”‚   prompt_replay_cooldown_steps=20,
â”‚   â”‚   prompt_replay_max_reuse_time=10,
â”‚   â”‚   prompt_replay_min_pass_rate=0.2,
â”‚   â”‚   prompt_replay_max_pass_rate=0.7,
â”‚   â”‚   exp_name='replay_8000_16rollouts_20cooldown_',
â”‚   â”‚   seed=123,
â”‚   â”‚   run_name='replay_8000_16rollouts_20cooldown___123__1765109231',
â”‚   â”‚   learning_rate=1e-06,
â”‚   â”‚   lr_scheduler_type='constant',
â”‚   â”‚   warm_up_steps=0,
â”‚   â”‚   warmup_ratio=0.0,
â”‚   â”‚   weight_decay=0.0,
â”‚   â”‚   set_weight_decay_on_bias_and_norm=True,
â”‚   â”‚   fused_optimizer=False,
â”‚   â”‚   per_device_train_batch_size=1,
â”‚   â”‚   total_episodes=512000,
â”‚   â”‚   world_size=1,
â”‚   â”‚   num_training_steps=1000,
â”‚   â”‚   local_eval_every=20,
â”‚   â”‚   save_freq=5000,
â”‚   â”‚   allow_world_padding=False,
â”‚   â”‚   backend_timeout=120,
â”‚   â”‚   response_length=3584,
â”‚   â”‚   temperature=1.0,
â”‚   â”‚   num_unique_prompts_rollout=32,
â”‚   â”‚   num_samples_per_prompt_rollout=16,
â”‚   â”‚   stop_strings=[],
â”‚   â”‚   async_steps=2,
â”‚   â”‚   num_epochs=1,
â”‚   â”‚   num_mini_batches=1,
â”‚   â”‚   beta=0.0,
â”‚   â”‚   clip_lower=0.2,
â”‚   â”‚   clip_higher=0.272,
â”‚   â”‚   truncated_importance_sampling_ratio_cap=2.0,
â”‚   â”‚   inflight_updates=True,
â”‚   â”‚   kl_estimator='kl3',
â”‚   â”‚   pack_length=4096,
â”‚   â”‚   masked_mean_axis=None,
â”‚   â”‚   masked_mean_denominator=None,
â”‚   â”‚   alpha=0.6,
â”‚   â”‚   ref_policy_update_freq=None,
â”‚   â”‚   load_ref_policy=True,
â”‚   â”‚   advantage_normalization_type='centered',
â”‚   â”‚   mask_truncated_completions=True,
â”‚   â”‚   active_sampling=True,
â”‚   â”‚   filter_zero_std_samples=True,
â”‚   â”‚   no_resampling_pass_rate=0.9,
â”‚   â”‚   record_entropy=True,
â”‚   â”‚   use_vllm_logprobs=False,
â”‚   â”‚   apply_r1_style_format_reward=False,
â”‚   â”‚   r1_style_format_reward=1.0,
â”‚   â”‚   additive_format_reward=False,
â”‚   â”‚   apply_verifiable_reward=True,
â”‚   â”‚   verification_reward=10.0,
â”‚   â”‚   remap_verifier=None,
â”‚   â”‚   llm_judge_model='azure/gpt-4o-mini-standard',
â”‚   â”‚   llm_judge_max_tokens=2048,
â”‚   â”‚   llm_judge_max_context_length=8192,
â”‚   â”‚   llm_judge_temperature=1.0,
â”‚   â”‚   llm_judge_timeout=60,
â”‚   â”‚   code_api_url='http://localhost:1234/test_program',
â”‚   â”‚   code_max_execution_time=1.0,
â”‚   â”‚   code_pass_rate_reward_threshold=0.0,
â”‚   â”‚   code_apply_perf_penalty=False,
â”‚   â”‚   max_length_verifier_max_length=32768,
â”‚   â”‚   non_stop_penalty=False,
â”‚   â”‚   non_stop_penalty_value=0.0,
â”‚   â”‚   single_gpu_mode=False,
â”‚   â”‚   num_learners_per_node=[1],
â”‚   â”‚   vllm_num_engines=1,
â”‚   â”‚   vllm_tensor_parallel_size=1,
â”‚   â”‚   vllm_enforce_eager=False,
â”‚   â”‚   vllm_sync_backend='nccl',
â”‚   â”‚   vllm_gpu_memory_utilization=0.9,
â”‚   â”‚   vllm_enable_prefix_caching=True,
â”‚   â”‚   vllm_top_p=1.0,
â”‚   â”‚   deepspeed_stage=2,
â”‚   â”‚   deepspeed_zpg=8,
â”‚   â”‚   deepspeed_offload_param=False,
â”‚   â”‚   deepspeed_offload_optimizer=False,
â”‚   â”‚   gather_whole_model=True,
â”‚   â”‚   enable_queue_dashboard=True,
â”‚   â”‚   queue_dashboard_port=None,
â”‚   â”‚   verbose=False,
â”‚   â”‚   update_progress_every=10,
â”‚   â”‚   with_tracking=True,
â”‚   â”‚   prompt_pass_table_dir='/home/rberger/rlvr/open-instruct/UC',
â”‚   â”‚   wandb_project_name='olmo3_curriculum',
â”‚   â”‚   wandb_entity=None,
â”‚   â”‚   push_to_hub=True,
â”‚   â”‚   hf_entity='ABaroian',
â”‚   â”‚   hf_repo_id='ABaroian/open_instruct_dev',
â”‚   â”‚   hf_repo_revision='replay_8000_16rollouts_20cooldown___123__1765109231',
â”‚   â”‚   hf_repo_url='https://huggingface.co/ABaroian/open_instruct_dev/tree/replay_8000_16rollouts_20cooldown___123__1765109231',
â”‚   â”‚   output_dir='/scratch-shared/rberber/rlvr/outputs/qwen25-math-rlzero-math/checkpoints/replay_8000_16rollouts_20cooldown___123__1765109231',
â”‚   â”‚   save_traces=False,
â”‚   â”‚   cache_dataset_only=False,
â”‚   â”‚   keep_last_n_checkpoints=3,
â”‚   â”‚   checkpoint_state_freq=5000,
â”‚   â”‚   checkpoint_state_dir='/scratch-shared/rberber/rlvr/outputs/qwen25-math-rlzero-math/checkpoints/state_20251207_130645',
â”‚   â”‚   gs_checkpoint_state_dir=None,
â”‚   â”‚   try_launch_beaker_eval_jobs_on_weka=False,
â”‚   â”‚   try_auto_save_to_beaker=True,
â”‚   â”‚   gs_bucket_path=None,
â”‚   â”‚   oe_eval_tasks=None,
â”‚   â”‚   oe_eval_max_length=4096,
â”‚   â”‚   oe_eval_beaker_image=None,
â”‚   â”‚   oe_eval_gpu_multiplier=None,
â”‚   â”‚   eval_priority='normal',
â”‚   â”‚   eval_workspace='ai2/tulu-3-results',
â”‚   â”‚   send_slack_alerts=False,
â”‚   â”‚   eval_on_step_0=True,
â”‚   â”‚   tools=None,
â”‚   â”‚   max_tool_calls=[5],
â”‚   â”‚   mask_tool_use=True,
â”‚   â”‚   only_reward_good_outputs=False,
â”‚   â”‚   number_documents_to_search=3,
â”‚   â”‚   search_api_endpoint=None,
â”‚   â”‚   code_tool_api_endpoint=None
â”‚   ),
â”‚   ModelConfig(
â”‚   â”‚   model_name_or_path='Qwen/Qwen2.5-Math-1.5B',
â”‚   â”‚   model_revision=None,
â”‚   â”‚   torch_dtype=None,
â”‚   â”‚   attn_implementation=None,
â”‚   â”‚   use_cache=False,
â”‚   â”‚   gradient_checkpointing=True,
â”‚   â”‚   use_peft=False,
â”‚   â”‚   lora_r=16,
â”‚   â”‚   lora_alpha=32,
â”‚   â”‚   lora_dropout=0.05,
â”‚   â”‚   lora_target_modules=None,
â”‚   â”‚   lora_modules_to_save=None,
â”‚   â”‚   lora_task_type='CAUSAL_LM',
â”‚   â”‚   load_in_8bit=False,
â”‚   â”‚   load_in_4bit=False,
â”‚   â”‚   bnb_4bit_quant_type='nf4',
â”‚   â”‚   use_bnb_nested_quant=False
â”‚   )
]
2025-12-07 13:07:17,968	INFO worker.py:1692 -- Using address local set in the environment variable RAY_ADDRESS
2025-12-07 13:07:20,512	ERROR services.py:1360 -- Failed to start the dashboard , return code 1
2025-12-07 13:07:20,512	ERROR services.py:1385 -- Error should be written to 'dashboard.log' or 'dashboard.err'. We are printing the last 20 lines for you. See 'https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#logging-directory-structure' to find where the log file is.
2025-12-07 13:07:20,513	ERROR services.py:1429 -- 
The last 20 lines of /scratch-shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/logs/dashboard.log (it contains the error message from the dashboard): 
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/home/rberger/.local/share/uv/python/cpython-3.12-linux-x86_64-gnu/lib/python3.12/multiprocessing/connection.py", line 399, in _recv
    raise EOFError
EOFError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/gpfs/home6/rberger/rlvr/open-instruct/.venv/lib/python3.12/site-packages/ray/dashboard/dashboard.py", line 297, in <module>
    loop.run_until_complete(dashboard.run())
  File "/home/rberger/.local/share/uv/python/cpython-3.12-linux-x86_64-gnu/lib/python3.12/asyncio/base_events.py", line 691, in run_until_complete
    return future.result()
           ^^^^^^^^^^^^^^^
  File "/gpfs/home6/rberger/rlvr/open-instruct/.venv/lib/python3.12/site-packages/ray/dashboard/dashboard.py", line 98, in run
    await self.dashboard_head.run()
  File "/gpfs/home6/rberger/rlvr/open-instruct/.venv/lib/python3.12/site-packages/ray/dashboard/head.py", line 405, in run
    handle.wait_for_module_ready()
  File "/gpfs/home6/rberger/rlvr/open-instruct/.venv/lib/python3.12/site-packages/ray/dashboard/subprocesses/handle.py", line 146, in wait_for_module_ready
    raise RuntimeError(
RuntimeError: Module MetricsHead failed to start. Received EOF from pipe.
2025-12-07 13:07:20,709	INFO worker.py:2013 -- Started a local Ray instance.
2025-12-07 13:07:26,536	INFO packaging.py:588 -- Creating a file package for local module '/gpfs/home6/rberger/rlvr/open-instruct'.
2025-12-07 13:07:28,141	INFO packaging.py:380 -- Pushing file package 'gcs://_ray_pkg_e0e2da71d1c15515.zip' (31.27MiB) to Ray cluster...
2025-12-07 13:07:28,174	INFO packaging.py:393 -- Successfully pushed file package 'gcs://_ray_pkg_e0e2da71d1c15515.zip'.
/gpfs/home6/rberger/rlvr/open-instruct/.venv/lib/python3.12/site-packages/ray/_private/worker.py:2052: FutureWarning: Tip: In future versions of Ray, Ray will no longer override accelerator visible devices env var if num_gpus=0 or num_gpus=None (default). To enable this behavior and turn off this error message, set RAY_ACCEL_ENV_VAR_OVERRIDE_ON_ZERO=0
  warnings.warn(
Waiting for placement group:   0%|          | 0/1 [00:00<?, ?it/s]
[33m(raylet)[0m warning: `VIRTUAL_ENV=/gpfs/home6/rberger/rlvr/open-instruct/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
[33m(raylet)[0m Using CPython 3.12.11
[33m(raylet)[0m Creating virtual environment at: .venv
[33m(raylet)[0m    Building open-instruct @ file:///gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515
[33m(raylet)[0m warning: `VIRTUAL_ENV=/gpfs/home6/rberger/rlvr/open-instruct/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[33m(raylet)[0m       Built open-instruct @ file:///gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515
[33m(raylet)[0m Installed 230 packages in 4.65s
Waiting for placement group: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.77s/it]
Waiting for placement group: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:19<00:00, 19.78s/it]

Getting master address:   0%|          | 0/1 [00:00<?, ?it/s]
[33m(raylet)[0m warning: `VIRTUAL_ENV=/gpfs/home6/rberger/rlvr/open-instruct/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead
Getting master address:   0%|          | 0/1 [00:07<?, ?it/s]

[36m(TemporaryActor pid=2194272)[0m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::PolicyTrainerRayProcess.__init__()[39m (pid=2194272, ip=145.136.62.102, actor_id=33c89b63d146e13137cd35f701000000, repr=<grpo_fast.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x1478b40d70b0>)
[36m(TemporaryActor pid=2194272)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TemporaryActor pid=2194272)[0m RuntimeError: The actor with name PolicyTrainerRayProcess failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:
[36m(TemporaryActor pid=2194272)[0m 
[36m(TemporaryActor pid=2194272)[0m [36mray::PolicyTrainerRayProcess.__init__()[39m (pid=2194272, ip=145.136.62.102, actor_id=33c89b63d146e13137cd35f701000000, repr=<grpo_fast.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x1478b40d70b0>)
[36m(TemporaryActor pid=2194272)[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TemporaryActor pid=2194272)[0m   File "/scratch-shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/open_instruct/utils.py", line 58, in <module>
[36m(TemporaryActor pid=2194272)[0m     import torch
[36m(TemporaryActor pid=2194272)[0m   File "/gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/__init__.py", line 2088, in <module>
[36m(TemporaryActor pid=2194272)[0m     _C._initExtension(_manager_path())
[36m(TemporaryActor pid=2194272)[0m                       ^^^^^^^^^^^^^^^
[36m(TemporaryActor pid=2194272)[0m   File "/gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/__init__.py", line 2084, in _manager_path
[36m(TemporaryActor pid=2194272)[0m     raise RuntimeError("Unable to find torch_shm_manager at " + path)
[36m(TemporaryActor pid=2194272)[0m RuntimeError: Unable to find torch_shm_manager at /gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/bin/torch_shm_manager
Traceback (most recent call last):
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/grpo_fast.py", line 4352, in <module>
    main(args, tokenizer_config, model_config)
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/grpo_fast.py", line 4216, in main
    create_model_and_optimizer(
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/grpo_fast.py", line 3037, in create_model_and_optimizer
    policy_group = ModelGroup(pg, PolicyTrainerRayProcess, args.num_learners_per_node, args.single_gpu_mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/grpo_fast.py", line 1993, in __init__
    results, _ = ray_get_with_progress(
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/utils.py", line 163, in ray_get_with_progress
    results[idx] = future.result()
                   ^^^^^^^^^^^^^^^
  File "/home/rberger/.local/share/uv/python/cpython-3.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/rberger/.local/share/uv/python/cpython-3.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::PolicyTrainerRayProcess.__init__()[39m (pid=2194272, ip=145.136.62.102, actor_id=33c89b63d146e13137cd35f701000000, repr=<grpo_fast.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x1478b40d70b0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: The actor with name PolicyTrainerRayProcess failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:

[36mray::PolicyTrainerRayProcess.__init__()[39m (pid=2194272, ip=145.136.62.102, actor_id=33c89b63d146e13137cd35f701000000, repr=<grpo_fast.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x1478b40d70b0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch-shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/open_instruct/utils.py", line 58, in <module>
    import torch
  File "/gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/__init__.py", line 2088, in <module>
    _C._initExtension(_manager_path())
                      ^^^^^^^^^^^^^^^
  File "/gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/__init__.py", line 2084, in _manager_path
    raise RuntimeError("Unable to find torch_shm_manager at " + path)
RuntimeError: Unable to find torch_shm_manager at /gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/bin/torch_shm_manager
Traceback (most recent call last):
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/grpo_fast.py", line 4352, in <module>
    main(args, tokenizer_config, model_config)
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/grpo_fast.py", line 4216, in main
    create_model_and_optimizer(
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/grpo_fast.py", line 3037, in create_model_and_optimizer
    policy_group = ModelGroup(pg, PolicyTrainerRayProcess, args.num_learners_per_node, args.single_gpu_mode)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/grpo_fast.py", line 1993, in __init__
    results, _ = ray_get_with_progress(
                 ^^^^^^^^^^^^^^^^^^^^^^
  File "/gpfs/home6/rberger/rlvr/open-instruct/open_instruct/utils.py", line 163, in ray_get_with_progress
    results[idx] = future.result()
                   ^^^^^^^^^^^^^^^
  File "/home/rberger/.local/share/uv/python/cpython-3.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "/home/rberger/.local/share/uv/python/cpython-3.12-linux-x86_64-gnu/lib/python3.12/concurrent/futures/_base.py", line 401, in __get_result
    raise self._exception
ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::PolicyTrainerRayProcess.__init__()[39m (pid=2194272, ip=145.136.62.102, actor_id=33c89b63d146e13137cd35f701000000, repr=<grpo_fast.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x1478b40d70b0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: The actor with name PolicyTrainerRayProcess failed to import on the worker. This may be because needed library dependencies are not installed in the worker environment:

[36mray::PolicyTrainerRayProcess.__init__()[39m (pid=2194272, ip=145.136.62.102, actor_id=33c89b63d146e13137cd35f701000000, repr=<grpo_fast.FunctionActorManager._create_fake_actor_class.<locals>.TemporaryActor object at 0x1478b40d70b0>)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scratch-shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/open_instruct/utils.py", line 58, in <module>
    import torch
  File "/gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/__init__.py", line 2088, in <module>
    _C._initExtension(_manager_path())
                      ^^^^^^^^^^^^^^^
  File "/gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/__init__.py", line 2084, in _manager_path
    raise RuntimeError("Unable to find torch_shm_manager at " + path)
RuntimeError: Unable to find torch_shm_manager at /gpfs/scratch1/shared/rberber/cache/ray_tmp/ray/session_2025-12-07_13-07-17_976433_2188828/runtime_resources/working_dir_files/_ray_pkg_e0e2da71d1c15515/.venv/lib/python3.12/site-packages/torch/bin/torch_shm_manager
[1;34mwandb[0m: ðŸš€ View run [33mreplay_8000_16rollouts_20cooldown___123__1765109231[0m at: [34mhttps://wandb.ai/RL_seminar/olmo3_curriculum/runs/xzki8kb6[0m
[1;34mwandb[0m: Find logs at: [1;35m../../../../gpfs/home6/rberger/rlvr/open-instruct/wandb/run-20251207_130712-xzki8kb6/logs[0m
